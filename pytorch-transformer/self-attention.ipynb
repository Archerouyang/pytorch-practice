{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0035, -0.8721, -0.4094, -0.0316],\n",
       "         [ 0.0035, -0.8722, -0.4094, -0.0316]],\n",
       "\n",
       "        [[-0.1021, -0.7288, -0.3341, -0.0718],\n",
       "         [-0.1018, -0.7292, -0.3345, -0.0717]],\n",
       "\n",
       "        [[ 0.0352, -0.8378, -0.3445, -0.1136],\n",
       "         [ 0.0345, -0.8374, -0.3438, -0.1138]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.query_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X shape : (batch_size, seq_len, hidden_dim)\n",
    "        Q = self.query_proj(X)\n",
    "        K = self.key_proj(X)\n",
    "        V = self.value_proj(X)\n",
    "       \n",
    "        attention_weights = torch.matmul(\n",
    "            # K needs to be (batch_size, hidden_dim, seq_len)\n",
    "            Q, K.transpose(1, 2)\n",
    "       )\n",
    "        # output (batch, seq, seq)\n",
    "\n",
    "        attention_weights = torch.softmax(attention_weights / math.sqrt(self.hidden_dim), dim = -1)\n",
    "\n",
    "        # (batch, seq, hidden)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        return output\n",
    "\n",
    "X = torch.rand(3, 2, 4)\n",
    "\n",
    "self_att_net = SelfAttentionV1(4)\n",
    "self_att_net(X)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiency Imporved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3425, -0.7286, -0.6025, -0.2077],\n",
       "         [ 0.3430, -0.7251, -0.5989, -0.2084]],\n",
       "\n",
       "        [[ 0.4270, -0.6466, -0.5041, -0.2572],\n",
       "         [ 0.4263, -0.6486, -0.5052, -0.2594]],\n",
       "\n",
       "        [[ 0.2814, -0.7100, -0.6775,  0.0388],\n",
       "         [ 0.2832, -0.7092, -0.6776,  0.0408]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SelfAttentionV2(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.proj = nn.Linear(hidden_dim, hidden_dim*3) # concat Q, K, V\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # X shape : (batch_size, seq_len, hidden_dim)\n",
    "        QKV = self.proj(X)\n",
    "        Q, K, V = torch.split(QKV, self.dim, dim = -1)\n",
    "       \n",
    "        attention_weights = torch.matmul(\n",
    "            # K needs to be (batch_size, hidden_dim, seq_len)\n",
    "            Q, K.transpose(1, 2)\n",
    "       )\n",
    "        # output (batch, seq, seq)\n",
    "\n",
    "        attention_weights = torch.softmax(attention_weights / math.sqrt(self.hidden_dim), dim = -1)\n",
    "\n",
    "        # (batch, seq, hidden)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        return output\n",
    "\n",
    "X = torch.rand(3, 2, 4)\n",
    "\n",
    "self_att_net = SelfAttentionV1(4)\n",
    "self_att_net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding dropout, mask, output_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionV3(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.query_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, X, attention_mask = None):\n",
    "        # X shape : (batch_size, seq_len, hidden_dim)\n",
    "        Q = self.query_proj(X)\n",
    "        K = self.key_proj(X)\n",
    "        V = self.value_proj(X)\n",
    "       \n",
    "        attention_weights = torch.matmul(\n",
    "            # K needs to be (batch_size, hidden_dim, seq_len)\n",
    "            Q, K.transpose(1, 2)\n",
    "       )\n",
    "        # output (batch, seq, seq)\n",
    "\n",
    "        attention_weights = attention_weights / math.sqrt(self.hidden_dim)\n",
    "        if attention_mask is not None:\n",
    "            attention_weights = attention_weights.marked_fill(\n",
    "                attention_mask == 0, float('-inf')\n",
    "            )\n",
    "        attention_weights = torch.sofmax(attention_weights, dim = -1)\n",
    "        attention_weights = self.attention_dropout(attention_weights)\n",
    "        # (batch, seq, hidden)\n",
    "        attention_results = torch.matmul(attention_weights, V)\n",
    "\n",
    "        output = self.output_proj(attention_results)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, head_dim, head_num, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        self.head_dim = hidden_dim//head_num\n",
    "        \n",
    "\n",
    "        self.query_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, X, attention_mask = None):\n",
    "        # X(b, s, h)\n",
    "\n",
    "        batch, seq, _ = X.size()\n",
    "\n",
    "        Q = self.query_proj(X)\n",
    "        K = self.key_proj(X)\n",
    "        V = self.value_proj(X)\n",
    "\n",
    "        # (b, s, h) => (b, head_num, s, head_dim)\n",
    "        q_state = Q.view(batch, seq, self.head_num, self.head_dim).transpose(1,2)\n",
    "        k_state = K.view(batch, seq, self.head_num, self.head_dim).transpose(1,2)\n",
    "        v_state = V.view(batch, seq, self.head_num, self.head_dim).transpose(1,2)\n",
    "\n",
    "        attention_weight = torch.matmul(\n",
    "            q_state, k_state.transpose(-1, -2)\n",
    "        )/math.sqrt(self.head_dim)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0, float(\"-inf\")\n",
    "            )\n",
    "        \n",
    "        attention_weight = torch.softmax(attention_weight, dim = -1)\n",
    "        attention_weight = self.attention_dropout(attention_weight)\n",
    "        output_mid = torch.matmul(\n",
    "            attention_weight*v_state\n",
    "        ) # (b, head_num, s, head_dim)\n",
    "\n",
    "        output_mid = output_mid.transpose(1,2).contiguous() # (b, s, head_num, head_dim)\n",
    "        output_mid = output_mid.view(batch, seq, -1) # fill out last dim with h = head_dim*head_num\n",
    "\n",
    "        output = self.output_proj(output_mid)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
