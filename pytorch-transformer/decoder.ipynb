{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block / layer\n",
    "class SimpleDecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, head_num, attention_dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        self.head_dim = hidden_dim // head_num # multi-head\n",
    "\n",
    "        # layer (mha, ffn)\n",
    "        # mha\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.o_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.drop_att = nn.Dropout(attention_dropout_rate)\n",
    "        self.att_ln = nn.LayerNorm(hidden_dim, eps = 0.0000001)\n",
    "\n",
    "        # ffn (increase dim -> decrease dim -> layer norm)\n",
    "        self.up_proj = nn.Linear(hidden_dim, hidden_dim * 4) \n",
    "        self.down_proj = nn.Linear(hidden_dim*4, hidden_dim)\n",
    "        self.act_fn = nn.GELU() # (improved ReLU)\n",
    "        self.drop_ffn = nn.Dropout(0.1)\n",
    "        self.ffn_ln = nn.LayerNorm(hidden_dim, eps = 0.0000001)\n",
    "\n",
    "    def attention_layer(self, query, key, value, attention_mask = None):\n",
    "        # output shape (b, s, h)\n",
    "        key = key.transpose(2,3) # (b, head_num, head_dim, seq)\n",
    "        attention_weight = torch.matmul(query, key) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.tril()\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0, float('-inf')\n",
    "\n",
    "            )\n",
    "        else:\n",
    "            attention_mask = torch.ones_like(\n",
    "                attention_weight\n",
    "            ).tril()\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0, float('-inf')\n",
    "            )\n",
    "        \n",
    "        attention_weight = torch.softmax(attention_weight)\n",
    "        attention_weight = self.drop_att(attention_weight)\n",
    "        mid_output = torch.matmul(attention_weight, value) # (b, head_num, seq, head_dim)\n",
    "        \n",
    "        mid_output = mid_output.transpose(1, 2).contiguous()\n",
    "        batch, seq, _, _ = mid_output.size()\n",
    "        mid_output = mid_output.view(batch, seq, -1) # hidden_dim\n",
    "\n",
    "        output = self.o_proj(mid_output)\n",
    "\n",
    "        return  output\n",
    "        \n",
    "    def mha(self, X, mask = None):\n",
    "        # (b, s, h) -> (b, head_num, s, head_dim)\n",
    "        batch, seq, _ = X.size()\n",
    "\n",
    "        Q = self.query_proj(X)\n",
    "        K = self.key_proj(X)\n",
    "        V = self.value_proj(X)\n",
    "\n",
    "        query = Q.view(batch, seq, self.head_num, self.head_dim).transpose(1,2)\n",
    "        key = K.view(batch, seq, self.head_num, self.head_dim).transpose(1,2)\n",
    "        value = V.view(batch, seq, self.head_num, self.head_dim).transpose(1,2)\n",
    "\n",
    "        output = self.attention_layer(query, key, value, mask) \n",
    "        # post norm (b, s, h)\n",
    "        return self.att_ln(X + output)\n",
    "    \n",
    "    def ffn(self, X):\n",
    "        up = self.up_proj(X)\n",
    "        up = self.act_fn(up)\n",
    "        down = self.down_proj(up)\n",
    "        \n",
    "        # dropout\n",
    "        down = self.drop_ffn(down)\n",
    "\n",
    "        # post layernorm\n",
    "        return self.ffn_ln(X + down)\n",
    "\n",
    "    def forward(self, X, attention_mask = None):\n",
    "        X = self.mha(X, attention_mask)\n",
    "        X = self.ffn(X)\n",
    "        return X\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.layer_list = nn.ModuleList(\n",
    "            [\n",
    "                SimpleDecoderLayer(64,8) for i in range(5)\n",
    "            ]\n",
    "        )\n",
    "        self.emb = nn.Embedding(12, 64)\n",
    "        self.out = nn.Linear(64,12)\n",
    "\n",
    "    def forward(self, X, mask = None):\n",
    "        # (b, s)\n",
    "        X = self.emb(X)\n",
    "        for i, l in enumerate(self.layer_list):\n",
    "            X = l(X, mask)\n",
    "        print(X.shape)\n",
    "        output = self.out(X)\n",
    "        return torch.softmax(output, dim = -1)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "x = torch.randint(low = 0, high = 12, size(3,4) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
